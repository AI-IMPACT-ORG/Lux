\section{Introduction}
\label{sec:introduction}

Symmetry is among the most powerful principles in physics. In fundamental theory it appears both as a direct symmetry, as a spontaneously broken form, as anomalies in quantisation and as explicit symmetry breaking terms. The main driver is Emmy Noether's celebrated theorem \cite{noether1918} that if symmetry is present in a system, conserved quantities exist. Since these conserved quantities are typically physically relevant, this has elevated symmetry to a defining feature of fundamental physics. Consequently, the Lagrangian field theory formalism is preferred over others, along with dimensional regularization for loop computations. Also in string theory the worldsheet induced infinite symmetry algebras are considered a benefit and a key feature of the system. The main argument for supersymmetry after many years of development as a symmetry of nature is still its mathematical simplicity.

It should be noted that Noether's work \cite{noether1918} also contains a clear differentiation between global and local symmetries, as well as converse theorem that says that. 

This has led to a situation where we can construct infinite families of 4D effective field theories that are all consistent and theoretically equivalent, while the experimental status of particle physics has changed relatively little. In other words, the standard model of particle physics is remarkably stable against discovery. Abandoning symmetry has been proposed many times over as a solution, so far without convincing candidates for actual physics, despite promising numerical results for lower D gravity theory as well as 4D gauge theory on the lattice. Here symmetry is emergent after taking away the regulator. 

In mathematics, a similar "bounty of models of logic" exists. This is because the hallmark of logic is that its results should be independent of the presentation used to obtain them. The key is that the space of morphisms between models is infinite, and the space of models is infinite. This is a direct consequence of the fact that the space of models is a model of the space of models, and the space of models is a model of the space of models, and so on. Natural language shows paucity of expressive power in the face of logic. 

There is also the issue of "presentations" of logic systems. Basically, this is the definition of the formal system and its satisfaction relation (basically defining "what is equal"). Confusingly to physicists - there is no "Lagrangian theory of logic" that generates as a mnemonic much of the observables through for instance Feynman graph perturbation theory. There are only models of logic. 

Since there are very many ways to rewrite a model, mathematicians tend to stick to a core presentation, a core set of axioms in, usually, ZFC set theory. A simple thought experiment shows how big this space is: just think of basic arithmetic. The usual presentations in terms of 10 "digits" is but one choice of many. We could for instance carve marks on a wall - the math stays the same, but the presentation is different. There is no reason other than convenience why ZFC set theory is the core presentation. It basically corresponds to the mathematical symmetry inherent in the concept of a set.


The space of rewritings of a logic theory that preserve the set of theorems that can be derived from it is infinite, but also fascinating: proof complexity is a useful model for general computational complexity. Apart from theorem-preserving morphisms, there are also general structure-preserving morphisms. These actually have the right structure to model the group of general diffeomorphisms that appear in general relativity, with theorems playing the role of "scalar invariants". In first order form, this can be made very precise. 

Finally, it should be noted that logic is extraordinarily powerful: just a few axioms are sufficient to generate powerful algebraic structures. This remarkable efficiency is exemplified by Boolean algebra, which requires only four axioms to capture all logical operations:

\begin{enumerate}
\item \textbf{Commutativity}: $a \vee b = b \vee a$ and $a \wedge b = b \wedge a$
\item \textbf{Associativity}: $(a \vee b) \vee c = a \vee (b \vee c)$ and $(a \wedge b) \wedge c = a \wedge (b \wedge c)$
\item \textbf{Distributivity}: $a \wedge (b \vee c) = (a \wedge b) \vee (a \wedge c)$ and $a \vee (b \wedge c) = (a \vee b) \wedge (a \vee c)$
\item \textbf{Complementarity}: $a \vee \neg a = 1$ and $a \wedge \neg a = 0$
\end{enumerate}

From these four axioms, one can derive the entire structure of Boolean algebra, including De Morgan's laws, absorption laws, and all logical equivalences. This demonstrates the profound efficiency of logical systems: minimal foundational assumptions generate maximal mathematical structure.

This efficiency principle extends far beyond Boolean algebra. Our computational framework leverages this same principle through the 6-ary connective $\mathsf{G}_6$, which unifies three distinct computational paradigms (Turing machines, lambda calculus, and path integrals) under a single generating function structure. Just as four Boolean axioms generate all logical operations, our framework shows how a single logical primitive can generate the entire spectrum of computational behavior through renormalization group flow.

The power of logic lies not merely in its axiomatic efficiency, but in its capacity for self-reference and self-evaluation. As we will demonstrate, the communication channel itself becomes a logical system, creating recursive structures that enable partial self-evaluation—a property that emerges naturally from the arity structure governing information flow in both logical inference and computational processes.  


We will argue for a natural thought framework to guide our thinking. The thought framework is actually based on an amalgam of Einstein's argument for special relativity and Shannon's information theory model, enriched with a fair knowledge of the result. Key is to model such that only theorems, proofs and derivations from axioms can be communicated. Observers have their own private formal language and access to a common formal language for communicating results as well as a common formal proof system to verify results step by step in that language.

The key insight is that the channel itself can be understood as a logical system, and the encoding/decoding operations can be viewed as logical transformations \cite{shannon1948}. This creates a self-referential structure where the system can reason about its own communication processes, leading to the emergence of partial self-evaluation capabilities that we will show are fundamentally connected to the S3xZ2 symmetry breaking mechanism.

Crucially, results should be invariant under proof-preserving morphisms. This is a natural requirement for any two possibly related formal systems, and it is a natural requirement for any physical system. Locally, this will look like proof-rewriting to some extent. Actually, proof length depends on order of proof steps, which makes this notion slightly ill-defined. The average number of proof steps for all proofs within a given computational bound on the common system however, is well-defined.

A fundamental requirement is that we separate the formal system and its theorems from its known derivations. These derivations may be local. Globally however, they may not be the shortest. Finally, given any theorem in the common language, it is a priori hard to gauge if the computation to derive this from its axioms finishes from data within the system. However, verifying a derivation in the common language is much easier, usually.

The role of fundamental limitations on truth and provability becomes particularly interesting when we consider both Gödel theorems \cite{godel1931}, as well as the Tarski undefinability theorem \cite{tarski1936}. Here we incorporate them into the framework by naturally restricting to the common language for communicating anything. This creates a natural "partiality" separation which will be made precise later. 

We show by construction that Robinson's Q system \cite{robinson1950}, together with the diagonal lemma \cite{kleene1952}, can be written in such a way it has an explicit Z2xS3 syntactic symmetry group. We show this leads in principle to an infinite number of constraints. One solution are the usual Gödel \cite{godel1931}, Löb \cite{lob1955} and Tarski \cite{tarski1936} theorems, which are symmetry permuted here. We argue that these can only be added to the system as axioms as axiom schemas. In general this symmetry is almost always broken by individual terms. We argue this can be modeled cleanly as left, right arity on symbols that have strict local interactions in sentences. This leads to notions of invariance, equivariance as well as co- and contravariance under local presentation transformations. We argue the resource system plays the role of ghost particles to restore the local presentation symmetry. We argue that this partiality is not a limitation but a feature, enabling the system to adapt and evolve while maintaining consistency within its defined boundaries—a property that emerges naturally from the arity structure that governs information flow in both logical inference and computational processes.

A natural Noether theorem analogue then naturally leads to generating functions for invariants of the system. We show the entire construction is consistent with the symmetries. 

The generating functions have a natural modular symmetry, that lifts to a notion of "modular typing", which we implement for general container/component classes (modular typing for container, static typing for components). We show how to implement this for natural "propagators" as well as two natural "three point vertices". We show the resulting system is consistent using computer algebra -  even though the derivation is admittedly not very strict in places, the result is definite and verifiable. 

We show the resulting system forms a birepresentation with the 5 axioms of NBG set theory \cite{von_neumann1925}. Moreover, we show that the proof systems coincide, when we construct a natural modular time-step operator and study its kernel in our system. This kernel coincides with the proof system of the NBG axioms, showing that NBG theorems are contained in our system - this instantly connects us to most of mathematics. We show there is a notion of spectrum for this operator and that it has a natural gap. We argue that for the computational interpretation, this shows the existence of algorithms that do not verifiably terminate \cite{turing1936}. The phase separation itself we show to be related to a fundamental difference between P and NP systems \cite{cook1971}. We argue for universality of this result. 

The time-step operator is naturally related to what looks like an RG Flow operator. Flow consistency in various parameters induces towers of metalogical constraints, and shows that the moduli space set up by the modular parameters is in a real sense flat. We argue this means the system contains an integrable hierarchy and should be termed an integrable logic system. We identify some of the current algebra floating around, including a timelike Liouville theory, as well as a G2xG2 based Kac-Moody algebra. We show that the system is a natural candidate for a "string theory" in the sense of "causal", discretization of a 2D conformal field theory. Since the formal system can be extended arbitrarily so far, this is intriguing, but certainly no proof of anything. It mainly shows the majesty of mathematical consistency. We argue that in general the class of integrable systems we have identified could serve as a non-perturbative definition of string and field theory.  

Finally in physics we speculate on the role of the time-step operator and its mass-gap as a solution to the mass-gap problem. We show analogues to Connes-Kreimer's Birkhoff decomposition \cite{connes_kreimer2000} of the renormalization group flow as well as reflection calculus. IR to UV consistency conditions can naturally be imposed as a version of S-duality. The consequence of integrable RG flow would be an immediate solution to the naturalness problem. We argue a rather funky identification of scaling and conformal time plays a natural role. We briefly show how the S-matrix may be recovered. 

The technology uncovered here has many applications, we touch upon a select few:

- We show a categorical lift of the whole construction is natural. This we relate directly to categorical Langlands duality.

- We show the operator identified as the time-step operator has a natural interpretation in the generalized Riemann hypothesis \cite{riemann1859}. We offer machine checkable analysis. 

- We briefly explore some mathematics theorems which are relevant to the theory of large language models and large language model training. This includes the universal translation of mechanizable formal languages theorem. We argue the local version of the Cook-Levin theorem \cite{cook1971} shows almost exactly the opposite result from the usual interpretation. We briefly comment on the role of order of derivations as well as the role of the metalogical constraints we have uncovered. 

- A special section is dedicated to scaling analysis of large language model training, where we touch upon the double dip phenomenon as well as the Chinchilla scaling laws.

The framework's broad applicability extends beyond pure mathematics to practical applications in AI and machine learning. As we will show, the same arity structure that governs logical inference also provides insights into large language model training, scaling laws, and the double dip phenomenon, revealing deep connections between the abstract mathematical framework and concrete computational systems.

Finally, we include a section where we explain how these results were generated. 

Much work is left to be done. Most urgent are checks of almost everything in the paper by people who are not me.
