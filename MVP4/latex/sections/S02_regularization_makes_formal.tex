\section{The Regulator View: Understanding Compilation}
\label{sec:regularization}

Having established the basic computational framework in Section~\ref{sec:computation-paradigms}, we now explore how regularization provides the mathematical foundation for making the formal expansions well-defined within the L/B/R structure.

\paragraph{Domain preamble.} From this section onward we step into the physics domain: the computational constructs from Section~\ref{sec:computation-paradigms} are now interpreted as fields and regulators in a Green's-function language. Every definition therefore has two readings—the logical one (unchanged) and the physical one recorded on the domain ledger.

\paragraph{Hand-off from Section~\ref{sec:computation-paradigms}}
This section assumes the following structure from Section~\ref{sec:computation-paradigms}:
\begin{itemize}
\item $\mathsf{Gen4$ primitive} with signature $B^4 \to B$ from our BNF grammar
\item L/B/R structure with equality hierarchy $\equiv_L, \equiv_B, \equiv_R, \equiv_{\text{loc}}, \equiv_{\text{meta}}, \equiv_\star$
\item Generating function $\mathcal{G}(z_1,z_2,z_3,z_4;\vec{q},\Lambda)$ from equation~\eqref{eq:generating-function-g6}
\item Grading parameters $\vec{q} = (q_1, q_2, q_3)$ encoding computational paradigms
\item Scale parameter $\Lambda > 0$ controlling computational precision
\item Correlator coefficients $\mathcal{Z}_{n,m,k,l}(\vec{q})$ as matrix elements in graded Fock basis
\end{itemize}

\paragraph{What this section adds}
We introduce:
\begin{itemize}
\item Regulator hierarchy with four levels of computational control within L/B/R structure
\item Normalization operator $N$ with idempotency $N^2 = N$ respecting equality hierarchy
\item Encoders/decoders $E_{i\to j}$ between computational paradigms via L/B/R boundaries
\item Termination observable $\tau \in (0,1]$ as paradigm-independent stopping criterion
\item Moduli space $\mathcal{M}$ of normalized programs up to equivalence respecting $\equiv_\star$ equality
\end{itemize}

The normalization step in our computational cycle corresponds precisely to choosing regulators that control the behavior of the $\mathsf{Gen4}$ primitive within the L/B/R structure.

\subsection{The Regulator View of Computation in L/B/R Structure}

The computational process encoding $\to$ operator application $\to$ normalization (regularization) $\to$ decoding corresponds directly to compilation within the L/B/R structure, where the normalization step plays a crucial role in making the formal expansions well-defined. The regularization map can be expressed as:
\[
\mathcal{R}_\Lambda: \mathsf{Gen4} \mapsto \mathsf{Gen4}^{\text{reg}} = \sum_{n,m,k,l\ge0}\frac{z_1^n z_2^m z_3^k z_4^l}{n!\,m!\,k!\,l!}\,\mathcal{Z}_{n,m,k,l}(\vec{q})\,\Lambda^{-(n+m+k+l)} \cdot \Theta(n+m+k+l \leq K)
\]
where $K \in \mathbb{N}$ is the degree cutoff (integer-valued) and $\Lambda \in \mathbb{R}_+$ is the physical scale. We use smooth regulators to avoid artifacts:
\[
\mathsf{Gen4}^{\text{reg}} = \sum_{n,m,k,l\ge0}\frac{z_1^n z_2^m z_3^k z_4^l}{n!\,m!\,k!\,l!}\,\mathcal{Z}_{n,m,k,l}(\vec{q})\,\Lambda^{-(n+m+k+l)} \cdot e^{-(n+m+k+l)/K}
\]
This avoids scheme dependence issues that arise with sharp cutoffs $\Theta(n+m+k+l \leq K)$ and respects the equality hierarchy $\equiv_L, \equiv_B, \equiv_R, \equiv_{\text{loc}}, \equiv_{\text{meta}}, \equiv_\star$.

\subsection{Regulator Hierarchy}

Our framework introduces a natural hierarchy of regulators that control the computational process, providing a systematic way to understand how different levels of regularization interact:

\begin{definition}[Regulator Hierarchy]
\label{def:regulator-hierarchy}
The regulator structure follows a natural hierarchy:
\begin{align}
\text{Conceptual regulator} &: \text{Boundary at } \infty \text{ (universal computation)} \\
\text{Operational regulator} &: \text{Overall scale parameter } \Lambda \\
\text{Grading parameters} &: \text{Three grading parameters } (q_1, q_2, q_3) \\
\text{State regulator} &: \text{Virasoro levels } n, m
\end{align}
\end{definition}

This hierarchy implements our framework's moduli space structure, with the boundary at infinity serving as the overall regulator and additional regulators emerging naturally from the generating function structure in equation~\eqref{eq:generating-function}.

We distinguish syntactic regulators (typing/arity, grammar) from semantic regulators (observable choice, evaluation budget).

\subsection{Termination Condition as Regularization}

\begin{definition}[Termination Observable]
\label{def:termination-observable}
Let $P_{\mathrm{vac}}:=|0\rangle\langle 0|$. Define the termination observable as a supermartingale:
\[
X_t := \langle \psi(t)|P_{\mathrm{vac}}|\psi(t)\rangle, \quad \text{where } |\psi(t)\rangle=e^{-it\hat T_{\mathrm{comp}}}|\psi_0\rangle
\]
A run halts at the first hitting time:
\[
T_{\text{halt}} := \inf\{t \geq 0 : X_t \geq \tau\}
\]
for a fixed $\tau\in(0,1]$. The supermartingale property ensures monotonicity: $\mathbb{E}[X_{t+s}|\mathcal{F}_t] \leq X_t$ for $s \geq 0$, preventing "re-unhalting" oscillations. This lives in the same observable space $\mathsf{Obs}$ used by $\mathcal{G}$ in §1; the threshold $\tau\in(0,1]$ is a normalization choice carried forward as a renormalization condition in §3.
\end{definition}

The termination condition effectively imposes a canonical normalization on the computational process. The evolution of the vacuum overlap follows:
\[
\frac{d}{dt}\langle \psi(t)|P_{\mathrm{vac}}|\psi(t)\rangle = -i\langle \psi(t)|[P_{\mathrm{vac}}, \hat{T}_{\mathrm{comp}}]|\psi(t)\rangle
\]
By requiring the projection onto the vacuum subspace to exceed the threshold $\tau$, we ensure that the computational state has evolved sufficiently close to a well-defined reference state, providing a natural stopping criterion that is independent of the specific computational paradigm. This condition plays a role analogous to convergence criteria in numerical analysis.

\subsection{Regularization Principles}

Regularization manifests differently across computational paradigms, but follows universal principles that transcend the specific implementation:

\paragraph{Two kinds of regularization.}
\textit{Syntactic} (typing/arity, grammar constraints) vs. \textit{Semantic} (observable choice, evaluation budget). Write $\llbracket\phi\rrbracket_{\Lambda}^{\text{syn},\text{sem}}$.

\begin{itemize}
\item Boundary conditions provide natural stopping criteria for computational processes
\item Normalization procedures ensure well-defined results across all paradigms
\item Scale parameters control the computational process through the generating function
\end{itemize}

\begin{definition}[Program to Coefficient Map]
\label{def:program-coefficient-map}
The map from programs to coefficients $\mathcal{Z}_{n,m}$ is given by:
\[
\mathcal{Z}_{n,m}(\vec{q}) = \sum_{\text{paths }p:\,|p|=(n,m)} w(p;\vec{q}),
\]
where $w(p;\vec{q})$ is the weight of path $p$ under grading $\vec{q}$, and
\[
\mathcal{G} = \sum_{n,m\ge0}\frac{z^n\bar{z}^{\,m}}{n!\,m!}\,\mathcal{Z}_{n,m}(\vec{q})\,\Lambda^{-(n+m)}.
\]
\end{definition}

Each paradigm implements these principles through different mathematical structures, yet they are unified by our generating function framework. The detailed paradigm-specific regularization procedures will be discussed in Section~\ref{sec:renormalization}.

\subsection{Axioms aligning computational views and universality up to normalization}

We consider four categories of programs/derivations:
$\mathbf{TM}$ (Turing/cellular), $\mathbf{\Lambda}$ (typed $\lambda$),
$\mathbf{Path}$ (path-integral amplitudes), and $\mathbf{Circ}$ (circuits/rewrites).
A normalization comonad $N$ on each category identifies presentations that differ by
gauge/scale conventions.

\begin{definition}[Normalization]
\label{def:normalization}
On each $\mathbf{C}\in\{\mathbf{TM},\mathbf{\Lambda},\mathbf{Path},\mathbf{Circ}\}$,
$N:\mathbf{C}\to\mathbf{C}$ is an idempotent comonad with:
\begin{itemize}
\item Counit: $\epsilon:N\Rightarrow \mathrm{Id}$ (natural transformation)
\item Comultiplication: $\delta:N\Rightarrow N\circ N$ (natural transformation)
\item Idempotency: $N\circ N = N$ (equality of functors)
\end{itemize}
The $N$-coalgebras $(X,\alpha:X\to NX)$ represent "gauge choices" where $\alpha$ is a section of $\epsilon_X$. Objects in the same $N$-coalgebra are identified up to gauge equivalence. The \emph{normalized} category is the Karoubi envelope $\mathbf{C}[N^{-1}]$ where morphisms are inverted if they become isomorphisms after applying $N$.
\end{definition}

\begin{axiom}[Encoding/decoding (essential surjectivity up to $N$)]
\label{ax:encdec}
There are functors $E_{i\to j}:\mathbf{C}_i\to\mathbf{C}_j$ with explicit natural isomorphisms:
\begin{align}
\eta_{i\to j} &: E_{j\to i}\circ E_{i\to j} \Rightarrow N_i \\
\eta_{j\to i} &: E_{i\to j}\circ E_{j\to i} \Rightarrow N_j
\end{align}
These satisfy the triangle identities:
\begin{align}
\epsilon_i \circ \eta_{i\to j} &= \mathrm{id}_{E_{i\to j}} \\
\epsilon_j \circ \eta_{j\to i} &= \mathrm{id}_{E_{j\to i}}
\end{align}
where $\epsilon_i, \epsilon_j$ are the counits of the normalization comonads.
\end{axiom}

\begin{axiom}[RG-compatibility]
\label{ax:rg}
Each $\mathbf{C}_i$ carries an RG endofunctor $\mathcal{R}_\Lambda$ with natural isomorphism:
\[
\zeta_{i\to j,\Lambda} : E_{i\to j}\circ \mathcal{R}_\Lambda \Rightarrow \mathcal{R}_\Lambda \circ E_{i\to j}
\]
This isomorphism is natural in $\Lambda$ and satisfies the semigroup law:
\[
\zeta_{i\to j,\Lambda_1 \cdot \Lambda_2} = \zeta_{i\to j,\Lambda_1} \circ \zeta_{i\to j,\Lambda_2}
\]
where $\Lambda$ acts as an object in $\mathsf{Scale}$ with strict monoidal action $(\mathbb{R}_+, \cdot, 1)$.
\end{axiom}

\begin{axiom}[Observable preservation]
\label{ax:obs}
There is a common observable functor $\mathcal{O}:\mathbf{C}_i\to\mathbf{Obs}$ s.t.
$\mathcal{O}\circ E_{i\to j}\cong \mathcal{O}$ and $\mathcal{O}\circ N\cong \mathcal{O}$.
\end{axiom}

\begin{axiom}[Conservativity on the base language]
\label{ax:cons}
Each $E_{i\to j}$ is conservative on a shared fragment (sub-logic) $\mathcal{L}_0$
(i.e. reflects isomorphisms/provability on $\mathcal{L}_0$).
\end{axiom}

\begin{conjecture}[Alignment and universality up to normalization]
\label{conj:alignment}
Under Axioms~\ref{ax:encdec}–\ref{ax:cons}, the normalized categories
$\mathbf{TM}[N^{-1}],\mathbf{\Lambda}[N^{-1}],\mathbf{Path}[N^{-1}],\mathbf{Circ}[N^{-1}]$
are equivalent:
\[
\mathbf{TM}[N^{-1}] \;\simeq\; \mathbf{\Lambda}[N^{-1}] \;\simeq\; \mathbf{Path}[N^{-1}] \;\simeq\; \mathbf{Circ}[N^{-1}].
\]
Equivalently, they embed as reflective subcategories of a universal glued category
$\mathcal{U}$ obtained as a bicategorical colimit of the four via the encoders.
\end{conjecture}

\paragraph{Motivation.}
This equivalence says that, \emph{up to normalization}, the four computational views
carry the same content; renormalization (Section~\ref{sec:rg-flow}) is then the natural
mechanism organizing this equivalence along scales.

\begin{definition}[Moduli of normalized programs]
\label{def:moduli}
Let $\mathfrak{P}$ be parameter space (couplings, encoders, presentation choices).
The moduli space is the quotient (stack) $\mathcal{M} := [\mathfrak{P} / \sim]$ where
$(\text{program},\text{data})\sim(\text{program}',\text{data}')$ iff they are related
by normalization $N$ and RG-preserving isomorphisms. Points $[P]\in\mathcal{M}$ are
\emph{universal programs} up to normalization.
\end{definition}

\subsection{Example: Peano universality across views}
\label{subsec:peano-moduli}
Let $\mathbf{PA}$ be the Lawvere theory of a natural numbers object (NNO)
with constants $0$ and successor $S$, and induction.

\paragraph{Four presentations.}
\begin{itemize}
\item $\mathbf{TM}$: a register machine for $S$ and primitive recursion on $\mathbb{N}$;
\item $\mathbf{\Lambda}$: Church numerals with $0,\;S,\;$ and a recursor $R$;
\item $\mathbf{Path}$: a path-integral model with amplitudes supported on unary steps $n\to n+1$;
\item $\mathbf{Circ}$: a symmetric monoidal presentation with generators $0,S$ and recursion combinators.
\end{itemize}

\paragraph{Encoders and normalization.}
There are encoders $E_{i\to j}$ (Section~\ref{conj:alignment}) sending each presentation to another,
with $E_{j\to i}\circ E_{i\to j}\simeq N$.
All four presentations define the same point $[\mathbf{PA}]\in\mathcal{M}$.

\begin{proposition}[Peano equivalence up to normalization]
Each presentation of $\mathbf{PA}$ yields the same normalized correlators for
the generating function $G$, i.e.\ $\mathcal{O}(\Lambda)$ (Def.~\ref{def:global-observable})
is invariant under encoders and normalization. Hence all four belong to the same moduli point.
\end{proposition}

\subsection{Summary and Outlook}

This section established the regulator view of computation:

\begin{enumerate}
\item Normalization step corresponds to choosing appropriate regulators
\item Regulator hierarchy controls computational process at multiple scales
\item Termination condition provides paradigm-independent normalization criterion
\item Traditional models are different regularization choices within our unified framework
\end{enumerate}

The regulator view unifies compile-time and runtime aspects through the generating function structure. The next section develops RG machinery showing how regulators evolve toward fixed points (Section~\ref{sec:rg-flow}).

\paragraph{Back to the computation ledger.} Whenever a regulator or observable is introduced here, we retain its computation-domain meaning: $\mathcal{R}_\Lambda$ is simultaneously a physical coarse-graining map and the logic transformer that will be used again when we discuss training dynamics (Section~\ref{sec:llm_rg}). We explicitly note these dual roles so that the logic statements that depend only on the computation ledger remain easy to identify.
