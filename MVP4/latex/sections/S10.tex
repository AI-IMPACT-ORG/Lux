\section{Learning as renormalisation of Correlators}
\label{sec:llm_rg}

This section presents a renormalisation approach to training observables in large language models, demonstrating how our computational framework applies to modern machine learning. In the physics domain, this corresponds to how effective field theories emerge from more fundamental descriptions through RG flow.

This section builds on the $\mathsf{Gen4}$ primitive from Section~\ref{sec:computation-paradigms}, the simplified equality hierarchy from Section~\ref{sec:formal-systems}, and the boundary maps from Section~\ref{sec:boundary-maps}. The objects defined in earlier sections are reused with domain-specific interpretations through systematic translation maps (see Table~\ref{tab:universal-domain-translation}). The LLM training process directly implements the Green's functions hierarchy through bare training correlators, dressed training dynamics, and renormalized model parameters.

\subsection{LLM Parameter Mapping}

The LLM moduli space $(N, D, C, T)$ maps explicitly to our computational parameters:

\begin{align}
\vec{q}_{\text{LLM}} &= (q_N, q_D, q_C) = (\log N, \log D, \log C) \label{eq:llm-q-map} \\
\Lambda_{\text{LLM}} &= T \quad \text{(training steps as RG scale)} \label{eq:llm-lambda-map} \\
\tau_{\text{LLM}} &= \text{convergence threshold} \quad \text{(termination observable)} \label{eq:llm-tau-map}
\end{align}

The training dynamics correspond to RG flow equations:
\begin{equation}
\frac{d\vec{q}_{\text{LLM}}}{dt} = \vec{\beta}_{\text{LLM}}(\vec{q}_{\text{LLM}}, T) \label{eq:llm-rg-flow}
\end{equation}
where $t = \log T$ and the beta functions encode how model parameters evolve during training.

\begin{definition}[Partial Stable Domain Maps (PSDM)]
\label{def:psdm}
Partial Stable Domain Maps (PSDM) are domain-specific evaluation functions that are defined only for programmes that halt within a regulator window $T$. Non-converging sequences yield undefined semantics, preserving the constructive nature of the global logic while allowing irreversible computation within domain ports. PSDMs provide the interface between the constructive core logic and domain-specific semantics.
\end{definition}

\textbf{PSDM Partiality}: The LLM domain port implements PSDM where evaluation is defined only for programmes that halt within the regulator window $T$.

\subsection{LLM Generating Function}

The LLM generating function connects to our foundational framework via:
\begin{equation}
\mathcal{G}_{\text{LLM}}(z, \bar{z}; \vec{q}_{\text{LLM}}, T) = \sum_{n,m\ge0}\frac{z^n\bar{z}^{\,m}}{n!\,m!}\,\mathcal{Z}_{n,m}^{\text{LLM}}(\vec{q}_{\text{LLM}})\,T^{-(n+m)} \label{eq:llm-generating-function}
\end{equation}
where $\mathcal{Z}_{n,m}^{\text{LLM}}(\vec{q}_{\text{LLM}})$ are the training correlators encoding the statistical properties of the model's predictions.

\subsection{Training Correlators and renormalisation}

We model output fluctuations by a field $\psi(x)$ with source $J$ probing correlations. The training correlators are defined as:

\begin{definition}[Training Correlators]
\label{def:training-correlators}
The $n$-point training correlators are:
\begin{equation}
  G_n(x_1,\ldots,x_n) = \frac{1}{Z[0]} \frac{\delta^n Z[J]}{\delta J(x_1) \cdots \delta J(x_n)} \bigg|_{J=0}
  \label{eq:n_point_correlator}
\end{equation}
These encode the statistical properties of the trained model's predictions.
\end{definition}

Bare and renormalised fields obey $\psi_B = Z_\psi^{1/2}\psi_R$, $\lambda_B=\mu^\varepsilon Z_\lambda \lambda_R$ where $\lambda$ parameterises nonlinearity/regularisation and $\mu$ is the reference scale.

\subsection{Callanâ€“Symanzik Equation for Training}

The training correlators satisfy the Callan--Symanzik equation:

\begin{assumption}[Training dynamics]
Training dynamics follow RG flow equations; beta and gamma functions are well-defined; renormalisation scale $\mu$ is fixed.
\end{assumption}

\begin{theorem}[Callan--Symanzik equation for training]
\label{thm:callan_symanzik_training}
The training correlators satisfy:
\begin{equation}
  \left(\mu \frac{\partial}{\partial \mu} + \beta_\lambda \frac{\partial}{\partial \lambda_R} + \gamma_\psi \right) G_n^R = 0
  \label{eq:callan_symanzik_training}
\end{equation}
where $\beta_\lambda$ and $\gamma_\psi$ are the beta and gamma functions for training dynamics.
\end{theorem}

\subsection{Scaling Laws and RG Fixed Points}

For GPT models, the empirical scaling law emerges from an RG fixed point:

\begin{example}[GPT scaling from RG fixed point]
\label{ex:gpt_rg_fixed_point}
For GPT models, the empirical scaling law:
\begin{equation}
  L(N,D,C) = \alpha N^{-\beta_N} D^{-\beta_D} C^{-\beta_C}
  \label{eq:gpt_scaling}
\end{equation}
emerges from an RG fixed point where the beta functions vanish. This corresponds to fixed-point couplings:
\begin{align}
  g_N^* &= -\beta_N = \Delta_\psi - \frac{d}{2} \quad \text{(model size scaling)} \label{eq:gpt-gn} \\
  g_D^* &= -\beta_D = \Delta_\psi - \frac{d}{2} + \gamma_\psi \quad \text{(data scaling)} \label{eq:gpt-gd} \\
  g_C^* &= -\beta_C = \Delta_\psi - \frac{d}{2} + \frac{1}{2}\gamma_\psi \quad \text{(compute scaling)} \label{eq:gpt-gc}
\end{align}
\end{example}

\begin{notation}[Hypotheses]
\label{not:hypotheses-llm}
\textbf{Assumptions:} Different LLM architectures exhibit distinct scaling dimensions; universality classes are well-defined; scaling behaviour is independent of implementation details.
\end{notation}

\begin{theorem}[Universality classes]
\label{thm:universality_classes_training}
Different LLM architectures belong to distinct universality classes characterised by their fixed point scaling dimensions:
\begin{itemize}
\item GPT class: $\Delta_\psi = 0.076$, $\gamma_\psi = 0.019$
\item BERT class: $\Delta_\psi = 0.065$, $\gamma_\psi = 0.020$
\item T5 class: $\Delta_\psi = 0.070$, $\gamma_\psi = 0.020$
\end{itemize}
Each class exhibits universal scaling behaviour independent of implementation details.
\end{theorem}

\subsection{MSRJD Representation and Stochastic Dynamics}

The Martin--Siggia--Rose--Janssen--de~Dominicis (MSRJD) representation \cite{martin1973,janssen1976,dedominicis1976} provides a principled action for stochastic gradient dynamics. The MSRJD action is:
\[
S[\psi] = \int \psi(x) \mathcal{L}[\psi](x) \, dx + \int J(x)\psi(x) \, dx
\]
where $\mathcal{L}$ is the stochastic differential operator and $J$ represents training data sources. This formalism maps training dynamics to a field theory with:

\begin{itemize}
\item Field $\psi(x)$: Model output fluctuations
\item Source $J$: Training data probing correlations
\item Action $S[\psi]$: Effective loss function
\item RG flow: Training dynamics toward fixed points
\end{itemize}

The MSRJD representation supplies the mathematical foundation for understanding training as a dynamical system governed by RG flow equations. Full stochastic calculus derivation in Appendix C.

\subsection{Connection to Universal Framework}

The application to LLMs demonstrates how our renormalisation framework provides a systematic approach to understanding modern AI systems through the lens of quantum field theory. The explicit parameter mappings ensure that LLM training dynamics are understood as a specific instance of our general computational RG flow, with training loss corresponding to the global observable $\mathcal{O}(\Lambda)$ and successful training corresponding to RG flow toward computational truth.

The next section explores the spectral gap theorem and its applications to number theory and function theory (Section~\ref{sec:spectral-gap}), completing the connection between our computational framework and fundamental mathematical structures.

\paragraph{Technical Details.} Detailed derivations of the MSRJD representation, beta function calculations, and scaling law derivations are provided in Appendix~\ref{app:llm-technical-derivations}.
