\begin{abstract}
Experimental results are formally dominant in fundamental science. In practice there are typically large tapestries of techniques to connect multiple theories to multiple experiments. In particle physics for instance a wealth of theories, techniques, formalisms and results exist based on various different approaches, paired with a wealth of experimental data. The actual bottleneck to progress seems to be connecting these two worlds more efficiently and effectively as the current pipelines are very long and sometimes convoluted. The chance that something is missing or has been missed in that pipeline is not zero. 

The greatest common denominator between the already closely aligned fields of physics, computation and mathematics is logic. Logic is designed to be independent of domain application. I.e. the logic of number theory is related to, but not necessarily the same as the logic of differential geometry or that of quantum field theory. In this article we show results of using AI tooling to construct logic systems systematically. A logic is a formal system with a notion of "truth". Since formal systems can be formulated inside mature computer algebra systems, this is a natural approach that provides a in-build sanity check on LLM "reasoning" to the strength of the implementation of then formal system. There are very close parallels between the construction of formal systems and the construction of QFT theories; both involve a choice of coordinates for one: essentially free fields for QFT and free logic symbols for logic. Interactions in logic are through definitions; in physics through coupling constants. The main difference is that logic is inherently exact by construction - if it is consistent. Consistent truth systems are severely restricted by powerfull, well-known theorems due to Gödel, Löb, Turing and Rice. These express fundamental constraints on maps of logic systems to themselves. These maps can not be total.  

As a first step along the effective logic for physics programm, we explore the theory of computation. Computation can be understood as the composition of encoding, function application(s), decoding as well as normalisation. Classically, normalisation can occur at any stage (i.e. commutes as an operator). This is the basic compiler pattern of the theory of programming languages. We argue that the classic Turing, Church and Feynman views on computation can be modelled as different, partial representations of the same basic logic theory. As a first step, we show how they all relate to each other using a novel generating functional approach deeply related to CFT conformal blocks. This is already somewhat computationally universal, which we demonstrate by showing equivalence between Church and Turing views in this framework: this is the well-known Church-Turing thesis. We conjecture a specialisation of this observation that encompasses all views on computation naturally. We show this pattern also naturally appears in the AGT correspondence. 

Central is the observation that the different parameter sets in this construction have to be mapped to each other, but the generating functionals are so far formal only. This is a problem as the normalisation step in the quantum case is no longer freely commuting. Hence we introduce (3+1) natural regulators, including an overall "scale" parameter, that make the formal expansions well-defined. We show there is a precise analogue of the usual renormalisation programm, including an extension of the RG equation to several commuting flows natural in the Toda hierarchy. This includes the use of normalisation conditions to fix the parameters to their semantic interpretation as natural numbers (semirings). Interestingly, there is an fundamental imbalance between the number of beta functions (3) and gamma functions (1). We conjecture natural generalisations of the a-functions and the c-functions through the AGT correspondence using the natural analog of the Fisher information metric (c theorem) and XXXXXXXX (a-theorem).

To explain this construction we construct a concrete logic system. A logic system describing chruch-style computation exists - this is the content of the Curry-Howard-Lambek correspondence. We argue we have here a generalisation of this correspondence to irreversible computation or, in other words, the general theory of programming languages. Basically, the theory of computation outlined above is conjectured to be a model of the logic system we construct in the paper. If our construction through computer algebra is consistent, then, we argue, this construction proves the conjecture. Verifying this is non-trivial, as the needed computer algebra is quite involved. For safety, we provide full implementations in agda, coq, isabelle, as well as a partial one in metamath.

In logic we argue these notions of regularisation and renormalisation conform to a very particular and peculiar conservative extension of basically any logic. This extension can be understood as a natural "hierarchical" deformation of the truth system of any logic, together with some of its subsystems. The deformation changes the truth system to a fundamentally asymmetric notion of equality (implication weighted by local weights), and we relate the undeformed truth to the kernel of a pair of natural ternary operator constructed inside the deformed logic that combine into an arity $(2,2)$ operator we call a "logic transformer". This logic transformer is basically a polymorphic generalization of the notion of a scaling operator in physics. Interestingly, this operator is known in the formal theory of programming languages as a "partial self-evaluation" operator directly tied to the notion of compilation, and in logic itself as a part of the diagonal lemma proof. In this construction the kernel of the transformer is naturally related to the set of all true theorems inside the undeformed logic. Moreover, we show that the spectrum of the logic transformer has a natural symmetry related gap between kernel and co-kernel. In computation, this is the difference between reversible and irreversible computation. In information theory, this is the basis of lossy compression. 

We show there is a natural notion of two boundaries-with-direction that arise inside the construction and show that there are two partial natural maps that can be constructed using the logic transformer as correlator. Holographic renormalization in the dS/CFT context is a natural interpretation of this, and this motivates the question if we can construct a system that admits two subsystems that are each other's boundaries as natural mirrors. The logic system we construct is exactly this system, at the threshold of logic consistency. In the renormalisation semantics this corresponds to showing the formal system is "renormalisable" - a notion intimately tied to the existence of a system wide truth system. 

We derive a swath of cross-checks will well-known results and conjectures in the literature. For instance, we prove a theorem inside the system that, we argue, generalises the Rice theorem. As corrolaries, Gödel, Löb, Turing and Tarksi results follow, as well as some well-knownresults in the PL literature. We argue for an analog of the Noether theorem (both theorems as well as the converse Noether Theorem), which we interpret as the consequence of global invariance of the logic of its presentation, and of local invariance of the logic of one of the sub-systems of its own interpretations. 

We prove a general theorem that basically states that the logic transformer provides a general translation mechanism between any two logics identified by a choice of four axioms with mild structural assumptions. We argue this is the natural way to think about the translation between logic and computation if one of the logics is identified as an actual physical representation of computation (i.e. a "CPU").

We work out a theory of computational complexity. Technically we identify that the Blum axioms hold for the generating functional of invariants. P vs NP for instance reduces to a topological classification of the logic transformer spectrum that directly connects to the symmetry-induced gap between kernel and co-kernel. In a domain map to theory of functions on number fields we argue the logic transformer provides a natural Hilbert-Polya operator, with a zeta-function interpretation of the natural heat-kernel regulator. We show this indeed conforms to the Hilbert-Polya scenario. We briefly discuss the role of the spectral gap in the context of the mass-gap theorem of quantum field theory. We argue these results are general for any domain map - if the logic checks out.

As a direct concrete application to experiment, we discuss the training of LLMs and point out that training scaling laws can be understood within this framework, as is the double dip phenomenon, which we argue is basically the discovery of effective (partial) representations of information through lossy compression. The technical engine of this is to study convolutions of the basic formal system as extensions of that system - this switches on additional coupling constants as an effective theory of computation. As formal systems, among others, define a formal language, this basically models large language models as (a controllable extension of) convolutions of a basic class of formal language models. We propose a stability analysis toolset closely aligned with RG flow ideas to study language model characteristics independent of model.

\end{abstract}
