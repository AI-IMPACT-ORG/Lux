\begin{abstract}
Experimental results are the bedrock of fundamental science. In practice there are typically large tapestries of techniques to connect multiple theories to multiple experimental results, leading to combinatorial explosion. In particle physics for instance a wealth of theories, techniques, formalisms and results exist based on various different approaches, paired with a wealth of experimental data. As the current pipelines are long and sometimes convoluted, we propose to use mathematical logic to connect the worlds of theory and of calculation more efficiently and effectively, and show this program is surprisingly actionable.

Logic is a very flexible modelling language, designed to provide consistent models independent of domain application. Mathematics, physics, computation and information theory are all built on logic, for instance. Logic can also be mechanised in computer algebra systems. We combine AI driven software engineering with these systems to close the "truth gap" to enable true LLM reasoning. As a result we can explore domain maps systematically. We argue that we can construct a special logic that allows for the construction of "generating functionals" as transseries that are consistent across all domains.

Any logic involves many modelling choices - the literature on this is vast. Consistent truth systems though are severely restricted by powerful, well-known theorems due to Gödel, Löb, Tarski, Turing, Rice and many others. These constraints relate to negation and self-reference. We show it is possible to construct a three-sublogic system embedded inside a constructive logic which has no global negation. Provocatively, we call the subsystems "Left", "Bulk" and "Right" and refer to the constructivist logic as "Meta". We show we can build these sublogics based on three tiny "local" semirings with a partial syntactic triality symmetry. We construct a partial guarded negation for each of the sublogics by using maps inside the logic itself that feature in the diagonal lemma. These maps identify the "Bulk" as a certain sum of the left and right boundaries that overlap inside Meta.

Motivated by analytic S-matrix considerations we first explore the theory of computation. Computation can be understood as the composition of encoding, function application(s), decoding as well as normalisation. Classically, normalisation can occur at any stage (i.e. commutes as an operator); in the quantum case this is the measurement step that defines the outcome. Since formal systems are basically formal languages with extra structure, this is the basic "compiler" pattern of the formal theory of programming languages, enriched with a computer algebra point of view for term normalisation.

We argue that the classic Turing, Church and Feynman views on computation can be modelled as different "partial" domain maps of the same basic logic theory united through the generating functional point of view. To make that claim precise, we first introduce regulation to make each system finite, and interpret the explicit parameter maps from the logic to a computational framework as a form of "renormalisation conditions". With this identification, renormalisation semantics aligns naturally with the local subsystems we construct. The novel generating functional approach is deeply related to path integrals and CFT conformal blocks. We argue these are literal analogues by interpreting the AGT correspondence as different stable domain maps from the same logic theory. These are related through computational universality.

We show a precise analogue of the usual renormalisation programme inside this logic can be constructed, where we treat regulation as a choice of gauge. This includes the use of normalisation conditions to fix the parameters to their semantic interpretation. We argue this supports a novel axiomatisation of analytic S-matrix theory with build-in renormalisation. We argue this supports any cosmological constant and aligns with holographic renormalisation in both AdS as well as dS/CFT contexts. The first aligns with the possibility to impose a global dagger operation. Finally, analogues of renormalisability can be formulated in this logic, leading to a conjecture that the constructed logic encompasses the full tractable fragment of first order logic. 

As our results amount to a re-axiomatisation of mathematics itself as a theory of computation, the verification burden is very high. We therefore provide extensive code in multiple formal languages.
To validate the logic system we construct a host of test suites, checking basic mathematical functionality of the system. We also recover very many known results from the literature, partially as corollaries of generalised theorems. For instance, we argue many classical results follow from a generalisation of Rice's theorem inside the meta logic. We argue the logic system generalises the Curry-Howard-Lambek correspondence to irreversible computation.

In a domain map to analytic number theory, we recover a logical variant of the Hilbert-Polya construction and show how generalised-Riemann-Hilbert shaped theorems arise as metatheorems. In the domain map to the theory of computation the same code provides extensive evidence for NP ≠ coNP and a theory of computational complexity. Conjecturally, in physics the gapped spectrum relates to the existence of a mass gap in quantum field theory. Some of the techniques used in some formal languages amount to the definition of a novel form of "modular threaded typing".

Finally we briefly discuss the use of the logic we construct to study more general first order logic systems as convolutions of the basic formal system. This leads to a natural hierarchy of first order logics, and a natural way to study the relationship between different first order logics. In the computation domain map this leads to a theory of software applications that has a very natural application to large language models—the weights of the convolution are mathematically precise models of the weights of the language model. This basically expresses natural language as a weighted convolution of a basic formal language model that, as a combined logic system, can be understood as an effective formal model. We derive some first applications of this for model-independent analysis of LLMs from a stability analysis of the RG flow equations. Some speculation for explanations of the double dip phenomenon, as well as training scaling laws are offered.
\end{abstract}
