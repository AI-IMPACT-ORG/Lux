% Definition of Renormalisation Conditions
% Extracted from ChatGPT conversation on renormalization logic

\section{RG Flow and Computational Behavior}
\label{sec:rg-flow-behavior}

The relationship between scale behavior and computational reversibility is determined by how scale parameter $\Lambda$ interacts with the grading parameters $\vec{q}$ introduced in Section \ref{sec:S01_computation_invitation}. This section establishes the RG flow isomorphism that determines when computations preserve information.

\subsection{RG Flow Behavior Classification}

\begin{definition}[RG Flow Behavior Classification]
\label{def:rg-flow-classification}
A computational system can exhibit different behaviors under RG flow as the scale parameter $\Lambda$ varies:
\begin{align}
\text{Converging flow} &: \lim_{\Lambda \to \infty} G(z, \bar{z}; \vec{q}, \Lambda) \text{ exists and is finite} \\
\text{Diverging flow} &: \lim_{\Lambda \to \infty} G(z, \bar{z}; \vec{q}, \Lambda) \text{ diverges to infinity} \\
\text{Marginal flow} &: \lim_{\Lambda \to \infty} G(z, \bar{z}; \vec{q}, \Lambda) \text{ oscillates, cycles, or exhibits other non-convergent behavior}
\end{align}
\end{definition}

\subsection{RG Map on Weights}

\begin{definition}[RG Map on Weights]
\label{def:rg-map}
For $b>1$, define $(\mathcal{R}_b\mathcal{Z})_{n,m}:=b^{-\Delta(n,m)}\,\mathcal{Z}_{\lfloor n/b\rfloor,\lfloor m/b\rfloor}$ for a chosen engineering dimension $\Delta$. This induces the RG flow on the generating function $G(z, \bar{z}; \vec{q}, \Lambda)$ defined in Section \ref{sec:S01_computation_invitation} through the isomorphism $\mathcal{R}_b: \mathcal{Z}_{n,m}(\vec{q}) \mapsto (\mathcal{R}_b\mathcal{Z})_{n,m}(\vec{q})$.
\end{definition}

\subsection{RG Flow-Computational Correspondence}

\begin{conjecture}[RG Flow-Computational Correspondence]
\label{conj:rg-computational-correspondence}
The relationship between RG flow behavior and computational properties can be described as:
\begin{align}
\text{Converging flow} &\leftrightarrow \text{Reversible computation} \quad (\text{information preserved}) \\
\text{Diverging flow} &\leftrightarrow \text{Irreversible computation} \quad (\text{information destroyed}) \\
\text{Marginal flow} &\leftrightarrow \text{Undecidable computation} \quad (\text{behavior cannot be determined})
\end{align}
\end{conjecture}

\subsection{Information-RG Flow Correspondence}

The RG flow behavior of the generating function directly encodes the information-theoretic properties of the underlying computation. This provides the mathematical foundation for understanding how computational processes preserve or destroy information.

\subsection{RG Flow Analysis}

\begin{proof}[RG Flow Analysis]
Converging RG flow preserves information and enables perfect reversibility. Diverging flow destroys information through nonlinear corrections. Marginal flow exhibits non-convergent behavior that cannot be classified - it could be infinite cycles, but there is no way to determine this from the flow itself.
\end{proof}

The scale parameter $\Lambda$ determines computational reversibility through the RG flow behavior of the generating function. This provides a natural definition of truth: a statement is true if and only if it corresponds to converging RG flow. The comprehensive treatment is developed in Section \ref{sec:complete-renormalization}.

\subsection{Unitarity and C-Function Conditions}

\begin{conjecture}[Reversibility vs. RG]
\label{conj:reversibility-rg}
Assume unitarity and existence of a monotone $C$â€“function along the induced flow $b\mapsto \mathcal{R}_b G$. Then convergence to a fixed point is equivalent to computational reversibility in the sense that the map on encodings is invertible a.e.; divergence implies irreversibility (information loss).
\end{conjecture}

These conditions provide the mathematical framework for understanding when computations can be reversed and when information is preserved or lost during the renormalisation process.
