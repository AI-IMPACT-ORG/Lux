% LLM Training and Scaling Laws
% Missing elements from the abstract - LLM training, scaling laws, double dip

\section{LLM Training and Scaling Laws}

\subsection{LLM Training as Convolution of Formal Systems}

\begin{definition}[Convolution of Formal Systems]
\label{def:convolution-formal}
Large Language Models can be understood as convolutions of basic formal systems. Given a base formal system $\mathcal{L}_0$, the convolution creates extensions:
\[
\mathcal{L}_n = \mathcal{L}_0 \star \mathcal{L}_0 \star \cdots \star \mathcal{L}_0 \quad \text{(n times)}
\]
where $\star$ represents the convolution operation that introduces additional coupling constants.
\end{definition}

\subsection{Effective Theory of Computation}

\begin{definition}[Effective Theory of Computation]
\label{def:effective-computation}
The convolution of formal systems switches on additional coupling constants, creating an effective theory of computation:
\begin{align}
\mathcal{L}_{\text{eff}} &= \mathcal{L}_0 + \sum_{i=1}^n g_i \mathcal{O}_i \\
\text{Coupling constants } g_i &: \text{Parameters learned during training} \\
\text{Operators } \mathcal{O}_i &: \text{Extensions of the base formal system}
\end{align}
\end{definition}

\subsection{Formal Language Models}

\begin{theorem}[LLMs as Formal Language Models]
\label{thm:llm-formal}
Large Language Models are (controllable extensions of) convolutions of a basic class of formal language models:
\begin{itemize}
\item \textbf{Base formal system}: Defines a formal language $\mathcal{L}_0$
\item \textbf{Convolution}: Creates extensions $\mathcal{L}_n$ with additional structure
\item \textbf{Training}: Learns the coupling constants $g_i$ that determine the effective theory
\item \textbf{Controllability}: The extensions are controllable through the coupling constants
\end{itemize}
\end{theorem}

\subsection{Training Scaling Laws}

\begin{definition}[Training Scaling Laws]
\label{def:scaling-laws}
The training scaling laws can be understood within the generating function framework. The scaling behavior follows:
\[
\text{Performance} \propto N^{\alpha} \cdot D^{\beta} \cdot C^{\gamma}
\]
where:
\begin{itemize}
\item $N$: Number of parameters (coupling constants)
\item $D$: Dataset size (training data)
\item $C$: Computational resources (RG flow iterations)
\item $\alpha, \beta, \gamma$: Scaling exponents determined by the effective theory
\end{itemize}
\end{definition}

\subsection{RG Flow Interpretation of Training}

\begin{theorem}[Training as RG Flow]
\label{thm:training-rg}
The training process can be interpreted as RG flow in the effective theory:
\begin{align}
\frac{dg_i}{dt} &= \beta_i(g_1, \ldots, g_n) \quad \text{(gradient descent)} \\
\frac{d\mathcal{L}_{\text{eff}}}{dt} &= \sum_i \beta_i \frac{\partial \mathcal{L}_{\text{eff}}}{\partial g_i}
\end{align}
where the beta functions $\beta_i$ are determined by the loss function and optimization algorithm.
\end{theorem}

\subsection{Double Dip Phenomenon}

\begin{definition}[Double Dip Phenomenon]
\label{def:double-dip}
The double dip phenomenon is the discovery of effective (partial) representations of information through lossy compression:
\begin{itemize}
\item \textbf{First dip}: Initial compression of information during early training
\item \textbf{Second dip}: Discovery of more efficient representations through deeper training
\item \textbf{Lossy compression}: Information is lost but essential structure is preserved
\end{itemize}
\end{definition}

\subsection{Information Compression and Loss}

\begin{theorem}[Information Compression]
\label{thm:information-compression}
The double dip phenomenon corresponds to information compression in the logic transformer framework:
\begin{itemize}
\item \textbf{Kernel}: Reversible computations preserve all information
\item \textbf{Co-kernel}: Irreversible computations compress information
\item \textbf{Spectral gap}: Measures the efficiency of compression
\end{itemize}
\end{theorem}

\subsection{Stability Analysis Toolset}

\begin{definition}[Stability Analysis Toolset]
\label{def:stability-toolset}
A stability analysis toolset closely aligned with RG flow ideas to study language model characteristics independent of model:
\begin{itemize}
\item \textbf{RG flow analysis}: Study how the effective theory evolves during training
\item \textbf{Stability criteria}: Determine when the training process is stable
\item \textbf{Universality classes}: Classify different types of language models
\item \textbf{Scaling behavior}: Predict performance scaling with model size
\end{itemize}
\end{definition}

\subsection{Language Model Universality Classes}

\begin{definition}[Language Model Universality Classes]
\label{def:lm-universality}
Different language models belong to different universality classes based on their RG flow behavior:
\begin{itemize}
\item \textbf{Class A}: Converging RG flow, stable training, predictable scaling
\item \textbf{Class B}: Diverging RG flow, unstable training, unpredictable scaling
\item \textbf{Class C}: Marginal RG flow, oscillatory training, complex scaling
\end{itemize}
\end{definition}

\subsection{Training Dynamics and RG Flow}

\begin{theorem}[Training Dynamics]
\label{thm:training-dynamics}
The training dynamics of language models follow RG flow equations:
\begin{align}
\frac{d\text{Loss}}{dt} &= -\sum_i \beta_i \frac{\partial \text{Loss}}{\partial g_i} \\
\frac{d\text{Performance}}{dt} &= \sum_i \gamma_i \frac{\partial \text{Performance}}{\partial g_i}
\end{align}
where the beta and gamma functions encode the training behavior.
\end{theorem}

\subsection{Scaling Laws and Information Theory}

\begin{theorem}[Scaling Laws Information]
\label{thm:scaling-information}
The scaling laws are directly related to information theory through the logic transformer:
\begin{itemize}
\item \textbf{Information capacity}: Determines the maximum model size
\item \textbf{Compression efficiency}: Determines the scaling exponents
\item \textbf{Information loss}: Determines the training stability
\end{itemize}
\end{theorem}

\subsection{Practical Applications}

\begin{remark}[Practical Applications]
\label{rem:practical-applications}
This framework provides practical tools for:
\begin{itemize}
\item \textbf{Model design}: Choose architectures that lead to stable RG flow
\item \textbf{Training optimization}: Use RG flow analysis to optimize training
\item \textbf{Performance prediction}: Predict scaling behavior from RG flow analysis
\item \textbf{Stability analysis}: Identify when training will be stable or unstable
\end{itemize}
\end{remark}

This framework provides a unified understanding of LLM training through the generating function structure, connecting it to renormalization group theory, information theory, and the logic transformer framework.
